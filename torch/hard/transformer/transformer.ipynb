{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Build a Transformer Model from Scratch\n",
    "\n",
    "## Objective\n",
    "Implement a **Transformer model** in PyTorch for sequence processing and prediction. The model should include an embedding layer, a Transformer encoder, and an output projection layer.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Implement Positional Encoding to inject sequence order into embeddings  \n",
    "Create sinusoidal positional encodings that are added to input embeddings to provide order information.\n",
    "\n",
    "2. Implement Multi-Head Self Attention mechanism  \n",
    "Apply attention in parallel across multiple heads to capture different representation subspaces.\n",
    "\n",
    "3. Linear projection of queries, keys, and values  \n",
    "Use a single linear layer to project input into concatenated Q, K, V tensors.\n",
    "\n",
    "4. Scaled dot-product attention  \n",
    "Compute attention scores by scaled dot product of queries and keys, followed by softmax and application to values.\n",
    "\n",
    "5. Output projection after head concatenation  \n",
    "Concatenate the outputs of all heads and project back to the original embedding dimension.\n",
    "\n",
    "6. Implement FeedForward layer used within Transformer blocks  \n",
    "Build a two-layer MLP with a ReLU activation in between to process each token independently.\n",
    "\n",
    "7. Connect components in a TransformerEncoderLayer with proper layer normalization and residual connections  \n",
    "Apply residual connections and layer normalization around the attention and feedforward sublayers.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Support padded input sequences for variable-length data.\n",
    "- Ensure the model handles batched inputs with correct tensor shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        assert not embed_dim % num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.w_qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.w_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def split_attention_heads(self, qkv: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        (B, S, 3 * D) -> (3, num_head, B, S, D)\n",
    "        Return Q, K, V\n",
    "        \"\"\"\n",
    "        B, S = qkv.shape[0], qkv.shape[1]\n",
    "        split_qkv = qkv.view(B, S, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, -1)\n",
    "        return split_qkv[0], split_qkv[1], split_qkv[2]\n",
    "    \n",
    "    def forward(self, x, causal_mask: bool = True):\n",
    "        qkv = self.w_qkv(x) # (B, S, 3 * num_head * D)\n",
    "        Q, K, V = self.split_attention_heads(qkv)\n",
    "\n",
    "        scaled_dot_prod = Q @ K.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        if causal_mask:\n",
    "            # scaled_dot_prod = torch.tril(scaled_dot_prod) ## NOTE: THIS IS WRONG\n",
    "            mask = torch.ones(qkv.shape[1], qkv.shape[1], device=x.device).tril()\n",
    "            # Set masked spots to -inf so that they will go to 0 after softmax\n",
    "            scaled_dot_prod = scaled_dot_prod.masked_fill_(mask == 0, float(\"-inf\")) \n",
    "\n",
    "        attention_weights = F.softmax(scaled_dot_prod, dim=-1) #  B, num_head, S, S\n",
    "\n",
    "        out_split = attention_weights @ V\n",
    "        out_merged = out_split.permute(0, 2, 1, -1).contiguous().view(qkv.shape[0], qkv.shape[1], -1)\n",
    "\n",
    "        assert out_merged.shape[-1] == (self.num_heads * self.head_dim)\n",
    "        return self.w_o(out_merged)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, ff_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.multi_head_attn1 = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.multi_head_attn2 = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        mha1_out = self.multi_head_attn1(x)\n",
    "        block1_out = self.layernorm(x + mha1_out)\n",
    "        \n",
    "        # Block 2\n",
    "        x2 = self.linear(block1_out)\n",
    "        mha2_out = self.multi_head_attn2(x2)\n",
    "        block2_out = self.layernorm(x + mha2_out)\n",
    "        return block2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10, 12])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "seq_length = 10\n",
    "num_samples = 100\n",
    "input_dim = 12\n",
    "attn = MultiHeadSelfAttention(input_dim, num_heads=4)\n",
    "X = torch.rand(num_samples, seq_length, input_dim)  # Random sequences\n",
    "out = attn(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "seq_length = 10\n",
    "num_samples = 100\n",
    "input_dim = 1\n",
    "X = torch.rand(num_samples, seq_length, input_dim)  # Random sequences\n",
    "y = torch.sum(X, dim=1)  # Target is the sum of each sequence\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = 1\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "ff_dim = 64\n",
    "output_dim = 1\n",
    "\n",
    "model = TransformerModel(input_dim, embed_dim, num_heads, num_layers, ff_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.5771\n",
      "Epoch [200/1000], Loss: 0.8907\n",
      "Epoch [300/1000], Loss: 0.6074\n",
      "Epoch [400/1000], Loss: 0.3587\n",
      "Epoch [500/1000], Loss: 0.1986\n",
      "Epoch [600/1000], Loss: 0.1157\n",
      "Epoch [700/1000], Loss: 0.0762\n",
      "Epoch [800/1000], Loss: 0.0629\n",
      "Epoch [900/1000], Loss: 0.0575\n",
      "Epoch [1000/1000], Loss: 0.0379\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for [[[0.6648573279380798], [0.6041934490203857], [0.3187063932418823], [0.9813531041145325], [0.09837877750396729], [0.3223891258239746], [0.3124500513076782], [0.36122316122055054], [0.8705818057060242], [0.4751177430152893]], [[0.569571316242218], [0.05407053232192993], [0.16180634498596191], [0.8140731453895569], [0.34717607498168945], [0.6788632273674011], [0.11463749408721924], [0.21608346700668335], [0.7405895590782166], [0.8521053194999695]]]: [[5.141801834106445], [5.020108699798584]]\n"
     ]
    }
   ],
   "source": [
    "# Testing on new data\n",
    "X_test = torch.rand(2, seq_length, input_dim)\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
